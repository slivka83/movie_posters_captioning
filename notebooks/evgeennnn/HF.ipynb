{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4485,
     "status": "ok",
     "timestamp": 1666808572370,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "hHwaXTSeLITe",
    "outputId": "6ccce300-a68f-4501-f62d-a00c2ac7e01a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.23.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3071,
     "status": "ok",
     "timestamp": 1666808575428,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "XVrmwrx5LfLO",
    "outputId": "115db1a1-a760-4919-8859-6272f36a78bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.14.0.dev20221026+cu116)\n",
      "Requirement already satisfied: sympy in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (3.0b1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from torch) (4.3.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from sympy->torch) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3555,
     "status": "ok",
     "timestamp": 1666808578976,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "7ZULF02idYIG",
    "outputId": "53ac9494-40af-4932-eaf6-ad3a212d086f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install pickle5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5367,
     "status": "ok",
     "timestamp": 1666808584338,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "jaogxAvXbaYQ",
    "outputId": "25b1345f-efec-4ece-86ab-36d80c1fce87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.23.3)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.70.13)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: packaging in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (10.0.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.10.1)\n",
      "Requirement already satisfied: dill<0.3.6 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.3.5.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (1.5.0)\n",
      "Requirement already satisfied: responses<0.19 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (2022.10.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from datasets) (4.64.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging->datasets) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.12)\n",
      "Requirement already satisfied: colorama in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.5)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2022.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1687,
     "status": "ok",
     "timestamp": 1666808586014,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "4-Lz6Bu_mUOw",
    "outputId": "d1bd37f6-d30f-4319-b653-fd48a99e487c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: jiwer in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: levenshtein==0.20.2 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from jiwer) (0.20.2)\n",
      "Requirement already satisfied: rapidfuzz<3.0.0,>=2.3.0 in c:\\users\\ussrer\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from levenshtein==0.20.2->jiwer) (2.13.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2756,
     "status": "ok",
     "timestamp": 1666808998440,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "HfRS2nbNLb_G",
    "outputId": "413c0a9e-a891-4ae5-fc21-7b0f608d2f84"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 498,
     "status": "ok",
     "timestamp": 1666808999354,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "OiZgmCbmdRwB",
    "outputId": "16595d76-d0ee-44fe-bdb7-8077b900ea90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data (23583, 11)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pickle\n",
    "\n",
    "global_path =  'D:/Courses spring/ML System Design/movie_posters_captioning/'\n",
    "karpathy_json_path = global_path + 'data/interim/films_df.pkl'\n",
    "empty_imges = global_path + 'notebooks/evgeennnn/' +  'empty_img.txt'\n",
    "\n",
    "with open(karpathy_json_path, \"rb\") as dt:\n",
    "      data = pickle.load(dt) \n",
    "\n",
    "      x = data['description'].apply(lambda x: len(x))\n",
    "      data = data[x.between(x.quantile(.1), x.quantile(.9))]\n",
    "#       f = open(empty_imges, \"r\")\n",
    "#       d = f.read()\n",
    "#       empty_img = list(map(int, d.split(', ')))\n",
    "#       print(\"empty_img\", len(empty_img))\n",
    "#       print(\"data\", data.shape)\n",
    "      \n",
    "#       mask = data['filmId'].isin(empty_img)\n",
    "#       data = data[~mask]\n",
    "\n",
    "      print(\"data\", data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "XdpAvKWkgfxf"
   },
   "outputs": [],
   "source": [
    "data.reset_index(inplace=True)\n",
    "data = data[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nU4GpWuIdmA9"
   },
   "outputs": [],
   "source": [
    "# data.to_csv(global_path + 'data/interim/films_df.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "S2yve7A6iFpt"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(data, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VpRMnVTWgvxG"
   },
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "test_df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_nBn7p6K-QC"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from transformers import VisionEncoderDecoderModel, ViTFeatureExtractor, AutoTokenizer, BertTokenizer\n",
    "\n",
    "# # model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n",
    "# model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "#     \"google/vit-base-patch16-224-in21k\", \"DeepPavlov/rubert-base-cased\"\n",
    "# )\n",
    "# feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "HWxhzq5DXM56"
   },
   "outputs": [],
   "source": [
    "global_path =  'D:/Courses spring/ML System Design/movie_posters_captioning/'\n",
    "image_path = global_path + '/data/img/2.jpg'\n",
    "\n",
    "\n",
    "# model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "# model.config.pad_token_id = tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gPVPJNt3pm5n"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "\n",
    "class IAMDataset(Dataset):\n",
    "    def __init__(self, root_dir, df,feature_extractor,tokenizer, max_target_length=512):\n",
    "        self.root_dir = root_dir\n",
    "        self.df = df\n",
    "        # self.processor = processor #feature_extractor\n",
    "        self.tokenizer = tokenizer\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_target_length = max_target_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # get file name + text \n",
    "        file_name = str(self.df['filmId'][idx]) + '.jpg'\n",
    "        text = self.df['description'][idx]\n",
    "        # prepare image (i.e. resize + normalize)\n",
    "        image = Image.open(self.root_dir + file_name).convert(\"RGB\")\n",
    "        image = image.resize([384, 384])\n",
    "        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n",
    "        # add labels (input_ids) by encoding the text\n",
    "       \n",
    "        labels = self.tokenizer(text, \n",
    "                                          truncation='longest_first',\n",
    "                                          padding=\"max_length\", \n",
    "                                          # return_tensors=\"pt\",\n",
    "                                          max_length=self.max_target_length).input_ids\n",
    "        # important: make sure that PAD tokens are ignored by the loss function\n",
    "        labels = [label if label != self.tokenizer.pad_token_id else -100 for label in labels]\n",
    "        encoding = {\"pixel_values\": pixel_values.squeeze(), \"labels\": torch.tensor(labels)}\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9372,
     "status": "ok",
     "timestamp": 1666809017393,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "u0hRDVhEiR7B",
    "outputId": "75db6fcb-4c47-4ba6-c82e-00d9846e911a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertLMHeadModel: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertLMHeadModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertLMHeadModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased and are newly initialized: ['bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrOCRProcessor, ViTFeatureExtractor, BertTokenizer, VisionEncoderDecoderModel\n",
    "processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-handwritten\")\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\", \"DeepPavlov/rubert-base-cased\"\n",
    ")\n",
    "\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
    "\n",
    "device = 'cuda:0'\n",
    "model.to(device)\n",
    "\n",
    "train_dataset = IAMDataset(root_dir=global_path + 'data/img/',\n",
    "                           df=train_df,\n",
    "                          #  processor=processor\n",
    "                           feature_extractor=feature_extractor,\n",
    "                           tokenizer=tokenizer\n",
    "                           )\n",
    "eval_dataset = IAMDataset(root_dir=global_path + 'data/img/',\n",
    "                           df=test_df,\n",
    "                          #  processor=processor\n",
    "                           feature_extractor=feature_extractor,\n",
    "                           tokenizer=tokenizer\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14,
     "status": "ok",
     "timestamp": 1666809017393,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "kxRDKhxrivN0",
    "outputId": "6c758b45-a19d-4a0b-a0cc-2a953ca3db63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 18866\n",
      "Number of validation examples: 4717\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(eval_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1666809017395,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "ifVpRiNxi2CX",
    "outputId": "d6ad330f-da33-459b-d99e-e008a3a81000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pixel_values torch.Size([3, 224, 224])\n",
      "labels torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "encoding = train_dataset[0]\n",
    "for k,v in encoding.items():\n",
    "    print(k, v.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "p87gLqKrlbHB"
   },
   "outputs": [],
   "source": [
    "image = Image.open(train_dataset.root_dir + str(train_df['filmId'][0]) + '.jpg').convert(\"RGB\")\n",
    "# image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tQBO6NIJkstk"
   },
   "outputs": [],
   "source": [
    "from transformers import VisionEncoderDecoderModel\n",
    "\n",
    "# model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-stage1\")\n",
    "# set special tokens used for creating the decoder_input_ids from the labels\n",
    "model.config.decoder_start_token_id = processor.tokenizer.cls_token_id\n",
    "model.config.pad_token_id = processor.tokenizer.pad_token_id\n",
    "# make sure vocab size is set correctly\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "\n",
    "# set beam search parameters\n",
    "model.config.eos_token_id = processor.tokenizer.sep_token_id\n",
    "model.config.max_length = 64\n",
    "model.config.early_stopping = True\n",
    "model.config.no_repeat_ngram_size = 3\n",
    "model.config.length_penalty = 2.0\n",
    "model.config.num_beams = 4\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    # num_train_epochs=3,\n",
    "    predict_with_generate=True,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    fp16=True, \n",
    "    output_dir=\"./\",\n",
    "    logging_steps=2,\n",
    "    save_steps=50,\n",
    "    eval_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1666809020743,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "_SsNX_4tkzFb",
    "outputId": "b6cb14b8-cc4e-47f5-ab18-b74da5f5ac9b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USSRER\\AppData\\Local\\Temp\\ipykernel_5692\\152175726.py:3: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library ðŸ¤— Evaluate: https://huggingface.co/docs/evaluate\n",
      "  cer_metric = load_metric(\"cer\")\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "\n",
    "cer_metric = load_metric(\"cer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ABZTDmLdk1jH"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    pred_str = processor.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = processor.tokenizer.pad_token_id\n",
    "    label_str = processor.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "\n",
    "    cer = cer_metric.compute(predictions=pred_str, references=label_str)\n",
    "\n",
    "    return {\"cer\": cer}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1338830,
     "status": "error",
     "timestamp": 1666810362821,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "jXZTffwIk3JU",
    "outputId": "0f9a5718-7fbb-4587-d2e7-bbc0b88eaecc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuda_amp half precision backend\n",
      "C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 18866\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 56598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3378, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Temp\\ipykernel_5692\\1026679545.py\", line 13, in <module>\n",
      "    trainer.train()\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 1500, in train\n",
      "    return inner_training_loop(\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 1742, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\trainer.py\", line 2496, in training_step\n",
      "    self.scaler.scale(loss).backward()\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py\", line 488, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 197, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "RuntimeError: Unable to find a valid cuDNN algorithm to run convolution\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 1997, in showtraceback\n",
      "    stb = self.InteractiveTB.structured_traceback(\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1112, in structured_traceback\n",
      "    return FormattedTB.structured_traceback(\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1006, in structured_traceback\n",
      "    return VerboseTB.structured_traceback(\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 859, in structured_traceback\n",
      "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 793, in format_exception_as_a_whole\n",
      "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 848, in get_records\n",
      "    return list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 597, in stack_data\n",
      "    yield from collapse_repeated(\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\utils.py\", line 84, in collapse_repeated\n",
      "    yield from map(mapper, original_group)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 587, in mapper\n",
      "    return cls(f, options)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 551, in __init__\n",
      "    self.executing = Source.executing(frame_or_tb)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 323, in executing\n",
      "    source = cls.for_frame(frame)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 247, in for_frame\n",
      "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 275, in for_filename\n",
      "    return cls._for_filename_and_lines(filename, lines)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 285, in _for_filename_and_lines\n",
      "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\stack_data\\core.py\", line 95, in __init__\n",
      "    super(Source, self).__init__(*args, **kwargs)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\executing\\executing.py\", line 228, in __init__\n",
      "    self.tree = ast.parse(ast_text, filename=filename)\n",
      "  File \"C:\\Users\\USSRER\\AppData\\Local\\Programs\\Python\\Python310\\lib\\ast.py\", line 50, in parse\n",
      "    return compile(source, filename, mode, flags,\n",
      "MemoryError\n"
     ]
    }
   ],
   "source": [
    "from transformers import default_data_collator\n",
    "\n",
    "# instantiate trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    tokenizer=feature_extractor   \n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    data_collator=default_data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1059,
     "status": "ok",
     "timestamp": 1666810777675,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "-TJccvXWawul",
    "outputId": "479cb801-a680-4efe-f449-231e2b3c6dd1"
   },
   "outputs": [],
   "source": [
    "max_length = 16\n",
    "num_beams = 4\n",
    "gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "images = []\n",
    "image_path = global_path + 'data/img/1.jpg';\n",
    "i_image = Image.open(image_path)\n",
    "i_image = i_image.resize([384,384])\n",
    "if i_image.mode != \"RGB\":\n",
    "  i_image = i_image.convert(mode=\"RGB\")\n",
    "\n",
    "# model.feat\n",
    "images.append(i_image)\n",
    "pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n",
    "pixel_values = pixel_values.to(device)\n",
    "\n",
    "output_ids = model.generate(pixel_values, **gen_kwargs)\n",
    "print(output_ids)\n",
    "print(tokenizer.vocab)\n",
    "preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "preds = [pred.strip() for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 401
    },
    "executionInfo": {
     "elapsed": 4061,
     "status": "ok",
     "timestamp": 1666810664595,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "ZhSJuOcDpdv7",
    "outputId": "ccd5d27d-9f58-4517-af19-e1cf509dd36a"
   },
   "outputs": [],
   "source": [
    "i_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5713,
     "status": "ok",
     "timestamp": 1666810387759,
     "user": {
      "displayName": "Elena Nikolaevskaya",
      "userId": "00260841992567360325"
     },
     "user_tz": -240
    },
    "id": "waO4e9cnURWv",
    "outputId": "c0a3cfaa-e72e-4868-a913-68252fdeebc9"
   },
   "outputs": [],
   "source": [
    "trainer.save_model(global_path + 'model_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QB-SckaibXbS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM+J7yvmuckt70sRuG4wKuY",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
